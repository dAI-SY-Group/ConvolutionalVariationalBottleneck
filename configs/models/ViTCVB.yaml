model:
  name: VBVisionTransformer
  transformer_block_mlp:
    width: 256
    bias: False
    activation_function:
      name: nn.GELU
      parameters: []
    regularization_layer:
      name: MaskDropout
      parameters: [0]
  embedding_patch_size: 4
  embedding_dropout_rate: 0.0
  transformer_layers: 4
  hidden_size: 256
  attention_heads: 16
  attention_dropout_rate: 0.0
  bias: False

  VB: 
    _class: FullyConvolutionalVariationalBottleneck
    K: adaptive
    beta: 0.1
    positions: [0]
    bias: False
    kernel_size: 3

training:
  loss: VBLoss
  lr_scheduler:
    metric: VBLoss
  early_stopping:
    metric: VBLoss
  glob:
    early_stopping:
      metric: VBLoss

attack:
  ignore_gradient_layers: ['decoder', 'layers.1', 'layers.2', 'layers.3', 'final_layer_norm', 'classifier']
  regularization: 0.0001